{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32c84b9a-ca56-4519-8ba0-d4ad4e82df07",
   "metadata": {},
   "source": [
    "<pre>\n",
    "Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its\n",
    "application.\n",
    "\n",
    "\n",
    "A min-max scalar is a common technique used in machine learning to preprocess and normalize data. It rescales the feature values to a specified range (usually between 0 and 1) by subtracting the minimum value and dividing by the range (max-min). This results in all feature values falling within the same range, making it easier to compare and interpret the data. \n",
    "\n",
    "Normalization using a min-max scalar can be beneficial in many scenarios, as it can help to prevent certain features from dominating the analysis due to their initially large values. However, it may not be the best choice in situations where the data contains outliers or extreme values, as these can skew the normalization process and lead to inaccurate results. Overall, the min-max scalar is a useful tool for preprocessing data for machine models, but it should be used with caution and combination with other normalization techniques as needed\n",
    "\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "86c0a3b7-ea1c-4745-9b10-487aaed7d1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "val=[10,2,7,9,10,8,30,8,78]\n",
    "\n",
    "ar=np.array(val)\n",
    "\n",
    "ar=ar.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a6c2d564-9682-47c4-afff-804eecc32815",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.10526316],\n",
       "       [0.        ],\n",
       "       [0.06578947],\n",
       "       [0.09210526],\n",
       "       [0.10526316],\n",
       "       [0.07894737],\n",
       "       [0.36842105],\n",
       "       [0.07894737],\n",
       "       [1.        ]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "scaler.fit_transform(ar)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4bdb361-c203-49d7-a478-07d6799e8d1f",
   "metadata": {},
   "source": [
    "<pre>\n",
    "Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling?\n",
    "Provide an example to illustrate its application.\n",
    "\n",
    "\n",
    "The Unit Vector technique, also known as Unit Norm or Normalization, is a feature scaling method that rescales the values of a feature to have a unit norm or magnitude. It is commonly used in machine learning and data preprocessing to bring all features to the same scale without distorting the original data distribution.\n",
    "\n",
    "To apply the Unit Vector technique, each feature vector is divided by its Euclidean norm, which is calculated as the square root of the sum of the squared values in the vector. The result is a vector with a magnitude or norm of 1, hence the term \"unit vector.\" This scaling method ensures that all feature vectors have the same scale and direction, making them comparable.\n",
    "\n",
    "A min-max scalar is a common technique used in machine learning to preprocess and normalize data. It rescales the feature values to a specified range (usually between 0 and 1) by subtracting the minimum value and dividing by the range (max-min). This results in all feature values falling within the same range, making it easier to compare and interpret the data. \n",
    "\n",
    "Normalization using a min-max scalar can be beneficial in many scenarios, as it can help to prevent certain features from dominating the analysis due to their initially large values. However, it may not be the best choice in situations where the data contains outliers or extreme values, as these can skew the normalization process and lead to inaccurate results. Overall, the min-max scalar is a useful tool for preprocessing data for machine models, but it should be used with caution and combination with other normalization techniques as needed\n",
    "\n",
    "Here's an example to illustrate the application of the Unit Vector technique:\n",
    "\n",
    "Suppose we have a dataset with two features, \"Age\" and \"Income,\" and we want to scale these features using the Unit Vector technique. The original values of the features are as follows:\n",
    "\n",
    "Age = [30, 45, 20, 50]\n",
    "Income = [50000, 60000, 40000, 70000]\n",
    "\n",
    "To apply the Unit Vector technique, we calculate the Euclidean norm for each feature vector and then divide each vector by its norm:\n",
    "\n",
    "Euclidean norm of Age = sqrt(30^2 + 45^2 + 20^2 + 50^2) = sqrt(7305) ≈ 85.51\n",
    "Euclidean norm of Income = sqrt(50000^2 + 60000^2 + 40000^2 + 70000^2) = sqrt(12500000000) ≈ 111803.40\n",
    "\n",
    "After scaling, the feature vectors become:\n",
    "\n",
    "Scaled Age = [30/85.51, 45/85.51, 20/85.51, 50/85.51] ≈ [0.35, 0.53, 0.23, 0.59]\n",
    "Scaled Income = [50000/111803.40, 60000/111803.40, 40000/111803.40, 70000/111803.40] ≈ [0.45, 0.54, 0.36, 0.63]\n",
    "\n",
    "As a result, both features now have a unit norm or magnitude of 1, allowing for fair comparison and avoiding dominance by either feature due to their original scale.\n",
    "\n",
    "In summary, the Unit Vector technique scales features by dividing each vector by its Euclidean norm, resulting in unit-norm vectors. It ensures all features have the same scale and direction. In contrast, Min-Max scaling rescales features to a specific range by subtracting the minimum value and dividing by the range.\n",
    "\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c78d922f-4d89-4728-b810-cd6c9f4bea1c",
   "metadata": {},
   "source": [
    "<pre>\n",
    "Q3. What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an\n",
    "example to illustrate its application.\n",
    "\n",
    "What is Principal Component Analysis(PCA)?\n",
    "Principal Component Analysis(PCA) technique was introduced by the mathematician Karl Pearson in 1901. It works on the condition that while the data in a higher dimensional space is mapped to data in a lower dimension space, the variance of the data in the lower dimensional space should be maximum. \n",
    "\n",
    "Principal Component Analysis (PCA) is a statistical procedure that uses an orthogonal transformation that converts a set of correlated variables to a set of uncorrelated variables.PCA is the most widely used tool in exploratory data analysis and in machine learning for predictive models. Moreover, \n",
    "PCA is an unsupervised learning algorithm technique used to examine the interrelations among a set of variables. It is also known as a general factor analysis where regression determines a line of best fit. \n",
    "The main goal of Principal Component Analysis (PCA) is to reduce the dimensionality of a dataset while preserving the most important patterns or relationships between the variables without any prior knowledge of the target variables. \n",
    "\n",
    "\n",
    "\n",
    "Suppose we have a dataset with eight features related to customer behavior: purchase amount, frequency of purchases, time spent on the website, number of products viewed, customer rating, customer engagement score, customer lifetime value, and customer satisfaction score. We want to reduce the dimensionality of this dataset using PCA.\n",
    "\n",
    "1. Data Preparation: Standardize the dataset by subtracting the mean and dividing by the standard deviation. This step ensures that all features are on a similar scale.\n",
    "\n",
    "2. Covariance Matrix and Eigenvalue Decomposition: Calculate the covariance matrix of the standardized dataset. Perform eigenvalue decomposition on the covariance matrix to obtain the eigenvectors and eigenvalues.\n",
    "\n",
    "3. Selection of Principal Components: Sort the eigenvectors based on their corresponding eigenvalues in descending order. The eigenvectors with the highest eigenvalues are the principal components. The eigenvalues represent the amount of variance explained by each principal component.\n",
    "\n",
    "4. Determine the Number of Principal Components: Decide how many principal components to retain based on the explained variance and the desired level of information retention. You can use the cumulative explained variance to make this decision.\n",
    "\n",
    "5. Dimensionality Reduction: Project the standardized dataset onto the selected principal components to obtain a reduced-dimensional representation of the data. The new dataset will have fewer dimensions, represented by the retained principal components.\n",
    "\n",
    "For example, after performing PCA, we determine that the first four principal components explain 90% of the variance in the dataset. We choose to retain these four principal components for dimensionality reduction.\n",
    "\n",
    "The reduced-dimensional dataset would consist of four principal components. Each data point would have four values, which are the projections of the original data onto these principal components. These new values capture the most significant variability in the original dataset.\n",
    "\n",
    "By reducing the dimensionality using PCA, we simplify the dataset while retaining most of the important information. The reduced dataset can be used for various purposes such as clustering, anomaly detection, or further analysis to gain insights into customer behavior patterns.\n",
    "\n",
    "\n",
    "\n",
    "</pre>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14d649c-578d-4808-bb30-06df7a923bb7",
   "metadata": {},
   "source": [
    "<pre>\n",
    "Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature\n",
    "Extraction? Provide an example to illustrate this concept.\n",
    "\n",
    "Principal Component Analysis (PCA) is a dimensionality reduction technique that aims to transform a dataset with a potentially high number of correlated variables into a lower-dimensional space while preserving the most important information or patterns in the data. It achieves this by identifying the principal components, which are new variables that are linear combinations of the original variables.\n",
    "\n",
    "Here's how PCA works in dimensionality reduction:\n",
    "\n",
    "1. Data Preparation: PCA begins with a dataset consisting of n observations (data points) and p variables/features. It is assumed that the data has been standardized or normalized to have zero mean and unit variance.\n",
    "\n",
    "2. Covariance Matrix: PCA calculates the covariance matrix of the standardized data. The covariance matrix represents the relationships between pairs of variables in the dataset. It provides information about how the variables vary together.\n",
    "\n",
    "3. Eigenvalue Decomposition: The next step is to perform eigenvalue decomposition on the covariance matrix. This decomposition results in eigenvectors and eigenvalues. The eigenvectors represent the directions or principal components of the data, while the corresponding eigenvalues represent the amount of variance explained by each principal component.\n",
    "\n",
    "4. Selection of Principal Components: The eigenvectors are sorted based on their corresponding eigenvalues in descending order. The eigenvector with the highest eigenvalue is the first principal component, the one with the second-highest eigenvalue is the second principal component, and so on. By selecting a subset of these principal components, it is possible to reduce the dimensionality of the dataset.\n",
    "\n",
    "5. Dimensionality Reduction: The final step is to project the original data onto the selected principal components. This projection reduces the dimensionality of the data, as the new dataset will have fewer variables (principal components) than the original dataset.\n",
    "\n",
    "Here's an example to illustrate the application of PCA in dimensionality reduction:\n",
    "\n",
    "Suppose we have a dataset with three variables: \"Height,\" \"Weight,\" and \"Age.\" We want to reduce the dimensionality of this dataset using PCA.\n",
    "\n",
    "Original Dataset:\n",
    "Height = [170, 165, 180, 155]\n",
    "Weight = [65, 60, 70, 50]\n",
    "Age = [30, 35, 40, 25]\n",
    "\n",
    "1. Standardization: We standardize the data by subtracting the mean and dividing by the standard deviation. This ensures that all variables have zero mean and unit variance.\n",
    "\n",
    "2. Covariance Matrix: We calculate the covariance matrix of the standardized data:\n",
    "\n",
    "Covariance Matrix:\n",
    "       Height   Weight    Age\n",
    "Height  1.00     0.87    -0.97\n",
    "Weight  0.87     1.00    -0.93\n",
    "Age    -0.97    -0.93     1.00\n",
    "\n",
    "3. Eigenvalue Decomposition: We perform eigenvalue decomposition on the covariance matrix:\n",
    "\n",
    "Eigenvalues:\n",
    "2.40\n",
    "0.02\n",
    "0.01\n",
    "\n",
    "Eigenvectors:\n",
    "[0.71, 0.70, -0.01]\n",
    "[0.70, -0.71, -0.00]\n",
    "[-0.03, -0.02, -1.00]\n",
    "\n",
    "4. Selection of Principal Components: We select the principal component with the highest eigenvalue (2.40) as the first principal component and the second principal component with the second highest eigenvalue (0.02).\n",
    "\n",
    "5. Dimensionality Reduction: We project the original data onto the selected principal components:\n",
    "\n",
    "Projected Dataset:\n",
    "Principal Component 1 = [1.32, 0.31, -1.33, -0.29]\n",
    "Principal Component 2 = [-0.08, 0.35, 0.35, -0.63]\n",
    "\n",
    "As a result, the dimensionality of the dataset has been reduced from three variables (Height, Weight, age)\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef722cdd-ccf9-465b-8c6f-a22b8e8de45b",
   "metadata": {},
   "source": [
    "<pre>\n",
    "Q5. You are working on a project to build a recommendation system for a food delivery service. The dataset\n",
    "contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to\n",
    "preprocess the data.\n",
    "\n",
    "Min-Max scaling, also known as Min-Max normalization, is a feature scaling technique used to rescale the values of a feature to a specific range, typically between 0 and 1. It is a linear transformation method that preserves the relative relationships between the data points.\n",
    "\n",
    "The formula for Min-Max scaling is as follows:\n",
    "\n",
    "scaled_value = (value - min_value) / (max_value - min_value)\n",
    "\n",
    "where:\n",
    "\n",
    "\"value\" is the original value of the feature\n",
    "\"min_value\" is the minimum value of the feature in the dataset\n",
    "\"max_value\" is the maximum value of the feature in the dataset\n",
    "The resulting scaled values will fall within the range [0, 1], where 0 represents the minimum value of the feature, and 1 represents the maximum value of the feature.\n",
    "\n",
    "Min-Max scaling is useful when the actual numerical values of the features are important and need to be preserved. It is commonly used in machine learning algorithms that require features to be on a similar scale to avoid dominance by features with larger magnitudes.\n",
    "\n",
    "\n",
    "If we consider price feature  range\n",
    "it could range between 200 to 5000\n",
    "\n",
    "rating will  range between 1 to 5 \n",
    "\n",
    "some Ml algorithms are based on distance based that will get affected by huge difference between range \n",
    "\n",
    "The goal of normalization is to transform features to be on a similar scale. This improves the performance and training stability of the model.\n",
    "The purpose of normalization is to transform data in a way that they are either dimensionless and/or have similar distributions.\n",
    "Normalization gives equal weights/importance to each variable so that no single variable steers model performance in one direction just because they are bigger numbers.\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed36d19-e291-4178-88ce-434160c657f4",
   "metadata": {},
   "source": [
    "<pre>\n",
    "Q6. You are working on a project to build a model to predict stock prices. The dataset contains many\n",
    "features, such as company financial data and market trends. Explain how you would use PCA to reduce the\n",
    "dimensionality of the dataset.\n",
    "\n",
    "\n",
    "To use PCA for reducing the dimensionality of the dataset in the context of predicting stock prices, you would follow these steps:\n",
    "\n",
    "1. Data Preparation: Gather the dataset that includes various features related to company financial data and market trends. Ensure that the dataset is properly formatted and standardized, as PCA requires standardized data.\n",
    "\n",
    "2. Feature Selection: Before applying PCA, it's a good practice to perform feature selection. Evaluate the relevance and importance of each feature for the prediction task. Remove any irrelevant or redundant features that may not contribute significantly to the prediction performance. This step helps reduce the computational burden and improve the interpretability of the model.\n",
    "\n",
    "3. Standardization: Standardize the remaining features in the dataset to have zero mean and unit variance. This is important because PCA is sensitive to the scale of the variables. Standardizing the data ensures that features with larger magnitudes do not dominate the PCA process.\n",
    "\n",
    "4. PCA Calculation: Apply PCA to the standardized dataset. PCA calculates the covariance matrix of the features and performs eigenvalue decomposition to obtain the principal components. Each principal component is a linear combination of the original features.\n",
    "\n",
    "5. Explained Variance: Analyze the explained variance ratio associated with each principal component. The explained variance ratio represents the proportion of the total variance in the dataset that is accounted for by each principal component. It helps in understanding the relative importance of each principal component.\n",
    "\n",
    "6. Selection of Principal Components: Determine the number of principal components to retain based on the explained variance ratio. You can set a threshold, such as retaining components that collectively explain a certain percentage of the total variance (e.g., 90% or 95%). Retaining fewer principal components helps reduce the dimensionality while retaining the most important information in the data.\n",
    "\n",
    "7. Dimensionality Reduction: Project the standardized dataset onto the selected principal components to obtain a reduced-dimensional representation of the data. The new features are the principal components, which capture the most significant variability in the original dataset.\n",
    "\n",
    "8. Model Training and Evaluation: Utilize the reduced-dimensional dataset to train and evaluate your stock price prediction model. Apply appropriate machine learning algorithms, such as regression or time series models, using the reduced feature space.\n",
    "\n",
    "By applying PCA for dimensionality reduction, you can address the curse of dimensionality, improve model performance, reduce overfitting, and potentially gain insights into the most influential factors affecting stock prices.\n",
    "</pre>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee61f4f1-0112-4626-82ad-c9f7f236ed46",
   "metadata": {},
   "source": [
    "Q7. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the\n",
    "values to a range of -1 to 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "66444928-a55b-4f6c-892b-bfb058cb5fb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original List: [[ 1]\n",
      " [ 5]\n",
      " [10]\n",
      " [15]\n",
      " [20]]\n",
      "Normalized List: [[0.        ]\n",
      " [0.21052632]\n",
      " [0.47368421]\n",
      " [0.73684211]\n",
      " [1.        ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "list = np.array([1, 5, 10, 15, 20]).reshape(-1,1)\n",
    "print('Original List:',list)\n",
    "scaler = preprocessing.MinMaxScaler()\n",
    "normalizedlist=scaler.fit_transform(list)\n",
    "print('Normalized List:',normalizedlist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87fc3763-c98c-4a8b-96f1-3fc2429e21f8",
   "metadata": {},
   "source": [
    "<pre>\n",
    "Q8. For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform\n",
    "Feature Extraction using PCA. How many principal components would you choose to retain, and why?\n",
    "\n",
    "\n",
    "To perform feature extraction using PCA on a dataset with features [height, weight, age, gender, blood pressure], we would follow these steps:\n",
    "\n",
    "1. Data Preparation: Standardize or normalize the dataset, ensuring that all features have zero mean and unit variance.\n",
    "\n",
    "2. Covariance Matrix and Eigenvalue Decomposition: Calculate the covariance matrix of the standardized data and perform eigenvalue decomposition.\n",
    "\n",
    "3. Selection of Principal Components: Sort the eigenvectors based on their corresponding eigenvalues in descending order. The eigenvectors with the highest eigenvalues are the principal components.\n",
    "\n",
    "4. Determine the Number of Principal Components: Decide how many principal components to retain based on the explained variance and the desired level of information retention.\n",
    "\n",
    "To determine the number of principal components to retain, we typically consider the cumulative explained variance. The cumulative explained variance is the sum of the explained variances of the retained principal components. It provides an indication of how much information is retained by each additional principal component.\n",
    "\n",
    "In practice, a common approach is to set a threshold for the cumulative explained variance, such as 90% or 95%. We choose the minimum number of principal components that achieve the desired cumulative explained variance. Retaining fewer principal components reduces the dimensionality of the data but may result in some loss of information.\n",
    "\n",
    "The specific choice of the number of principal components to retain depends on the dataset and the application's requirements. A general guideline is to aim for a cumulative explained variance that retains a high amount of information while keeping the dimensionality low.\n",
    "\n",
    "Here's an example of how the number of principal components could be determined for the given dataset:\n",
    "\n",
    "1. Data Preparation: Standardize the dataset.\n",
    "\n",
    "2. Covariance Matrix and Eigenvalue Decomposition: Calculate the covariance matrix and perform eigenvalue decomposition.\n",
    "\n",
    "3. Selection of Principal Components: Sort the eigenvectors based on their corresponding eigenvalues.\n",
    "\n",
    "4. Determine the Number of Principal Components: Calculate the explained variance ratio for each principal component and the cumulative explained variance.\n",
    "\n",
    "Suppose the calculated cumulative explained variance is as follows:\n",
    "\n",
    "Principal Component 1: Explained Variance = 0.6\n",
    "Principal Component 2: Explained Variance = 0.25\n",
    "Principal Component 3: Explained Variance = 0.1\n",
    "Principal Component 4: Explained Variance = 0.04\n",
    "Principal Component 5: Explained Variance = 0.01\n",
    "\n",
    "Based on these explained variances, we calculate the cumulative explained variance as we add more principal components:\n",
    "\n",
    "Cumulative Explained Variance:\n",
    "Principal Component 1: 0.6\n",
    "Principal Components 1-2: 0.6 + 0.25 = 0.85\n",
    "Principal Components 1-3: 0.6 + 0.25 + 0.1 = 0.95\n",
    "Principal Components 1-4: 0.6 + 0.25 + 0.1 + 0.04 = 0.99\n",
    "Principal Components 1-5: 0.6 + 0.25 + 0.1 + 0.04 + 0.01 = 1.0\n",
    "\n",
    "In this example, if we aim for a cumulative explained variance of 90%, we would choose to retain the first three principal components (0.6 + 0.25 + 0.1 = 0.95). These three principal components capture 95% of the variance in the original dataset.\n",
    "\n",
    "The number of principal components to retain can be adjusted based on the desired trade-off between dimensionality reduction and information retention.\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c34d05-6a44-4155-99d4-c480cfd9b81b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
